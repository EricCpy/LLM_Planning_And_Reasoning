{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import markdownify\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import LogitsProcessorList\n",
    "from lmformatenforcer import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conference(BaseModel):\n",
    "    name: str = Field(description=\"Name of the conference\")\n",
    "    location: Optional[str] = Field(description=\"Location of the conference\")\n",
    "    date: Optional[str] = Field(description=\"Date of the location in the following format DD/MM/YYYY\")\n",
    "\n",
    "class Conferences(BaseModel):\n",
    "    conferences: list[Conference] = Field(description=\"List of Conferences\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: list[str] = Field(description=\"Google Search Query for Topic\")\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "def get_top_urls_for_google_search_query(query: str, number: int = 5) -> list[dict]:\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    results_div = soup.find(\"div\", id=\"rso\") # list of urls inside this elem\n",
    "    if not results_div:\n",
    "        print(\"FAILED BECAUSE GOOGLE SEARCH OUTPUT CHANGED!\")\n",
    "        return []\n",
    "    \n",
    "    output = []\n",
    "    for a_tag in results_div.find_all(\"a\", href=True):\n",
    "        title_tag = a_tag.find(\"h3\")\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text()\n",
    "            url = a_tag['href']\n",
    "            output.append({\"title\": title, \"url\": url})\n",
    "            if len(output) >= number:\n",
    "                break\n",
    "\n",
    "    return output\n",
    "\n",
    "def scrape_and_convert_to_markdown(urls: list[str]) -> list[str]:\n",
    "    markdown_texts = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        try:\n",
    "            markdown_text = markdownify.markdownify(str(soup))\n",
    "            markdown_texts.append(markdown_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return markdown_texts\n",
    "    \n",
    "def llamacpp_with_character_level_parser(llm: Llama, prompt: str, character_level_parser: Optional[CharacterLevelParser]) -> str:\n",
    "    logits_processors: Optional[LogitsProcessorList] = None\n",
    "    if character_level_parser:\n",
    "        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm, character_level_parser)])\n",
    "    \n",
    "    output = llm(prompt, logits_processor=logits_processors, max_tokens=4096)\n",
    "    text: str = output['choices'][0]['text']\n",
    "    return text\n",
    "\n",
    "def ask_llm_for_conferences(llm: Llama, markdown_texts: list[str]) -> list[Conference]:\n",
    "    conferences = dict()\n",
    "    for idx, text in enumerate(markdown_texts):\n",
    "        print(f\"Working on {idx}/{len(markdown_texts)}:\")\n",
    "        prompt = f\"\"\"You are an AI assistant. Based on the scraped search result below, extract and provide the top academic AI conferences in the format of a JSON list.\n",
    "                    Scraped Content:\n",
    "                    {text}\n",
    "                    You MUST answer using the following json schema: {Conferences.model_json_schema()}\"\"\"\n",
    "        try:\n",
    "            output = llamacpp_with_character_level_parser(llm, prompt, JsonSchemaParser(Conferences.model_json_schema()))\n",
    "            output_conferences = Conferences.model_validate_json(output)\n",
    "            for conference in output_conferences.conferences:\n",
    "                conferences[conference.name] = conference\n",
    "            print(\"Finished successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "    return list(conferences.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model_path = hf_hub_download(repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\", filename=\"Llama-3.2-3B-Instruct-Q6_K_L.gguf\") \n",
    "llm = Llama(model_path=downloaded_model_path, n_ctx=16384, n_threads=8, n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top AI conferences 2025',\n",
       " 'best AI conferences 2025',\n",
       " 'leading AI conferences 2025',\n",
       " 'top-tier AI conferences 2025',\n",
       " 'notable AI conferences 2025']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Generate five different Google search queries to find information on the top academic AI conferences in 2025.\n",
    "            These queries should be tailored to discover high-impact conferences covering fields like machine learning, natural language processing,\n",
    "            computer vision, and other areas within AI.\n",
    "            You MUST answer using the following json schema: {Queries.model_json_schema()}\"\"\"\n",
    "queries = llamacpp_with_character_level_parser(llm, prompt, JsonSchemaParser(Queries.model_json_schema()))\n",
    "queries = Queries.model_validate_json(queries).queries\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_urls = {url['url'] for query in queries for url in get_top_urls_for_google_search_query(query)}\n",
    "markdowns = scrape_and_convert_to_markdown(top_urls)\n",
    "conferences : list[Conference] = ask_llm_for_conferences(llm, markdowns)\n",
    "conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"conferences\":[{\"name\":\"abc\",\"location\":\"newyork\",\"date\":\"11/12/2023\"}]}'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write conferences to file\n",
    "output_path = os.path.join(os.getcwd(), \"conferences\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "with open(os.path.join(output_path, f\"conferences_{llm.metadata['general.name']}.json\"), 'w', encoding='utf-8') as f:\n",
    "    f.write(Conferences(conferences=conferences).model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = os.path.join(os.getcwd(), \"downloads\")\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.set_preference(\"browser.download.folderList\", 2)  # custom location\n",
    "options.set_preference(\"browser.download.dir\", download_path)\n",
    "options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"text/csv\")\n",
    "options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "def get_number_of_searches(query: str):\n",
    "    url = f\"https://trends.google.com/trends/explore?date=today%201-m&q={query.replace(' ', '%20')}\"\n",
    "    driver.get(url)\n",
    "    download_button = None\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            download_button = driver.find_element(By.XPATH, \"//button[@class='widget-actions-item export' and @title='CSV']\")\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            time.sleep(1)\n",
    "            driver.refresh()\n",
    "\n",
    "    if download_button:\n",
    "        download_button.click()\n",
    "    else:\n",
    "        print(\"Download button not found after multiple attempts.\")\n",
    "        return 0 \n",
    "\n",
    "    csv_file_path = os.path.join(download_path, 'multiTimeline.csv')\n",
    "    timeout = 5\n",
    "    start_time = time.time()\n",
    "    while not os.path.exists(csv_file_path):\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(\"CSV file download timed out.\")\n",
    "            return 0 \n",
    "        time.sleep(1)\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, skiprows=2) # first two rows are header and not csv\n",
    "    total_searches = df.iloc[:, 1].sum()\n",
    "    os.remove(csv_file_path)\n",
    "    return total_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferences by Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total searches for 'Data + AI Summit': 1038\n"
     ]
    }
   ],
   "source": [
    "conferences_popularity :dict[str, tuple[Conference, int]]= dict()\n",
    "for conference in conferences:\n",
    "    total_searches = get_number_of_searches(conference.name)\n",
    "    conferences_popularity[conference.name] = (conference, total_searches)\n",
    "    \n",
    "conferences_sorted_by_searches = sorted(conferences_popularity.values(), key=lambda x: x[1], reverse=True)\n",
    "for conference, searches in conferences_sorted_by_searches:\n",
    "    print(f\"Conference: {conference.name}, Location: {conference.location}, Date: {conference.date}, Searches: {searches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferences by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%d/%m/%Y\") if date_str else None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "valid_conferences = [\n",
    "    conference for conference in conferences if parse_date(conference.date) is not None\n",
    "]\n",
    "conferences_sorted_by_date = sorted(valid_conferences, key=lambda conf: parse_date(conf.date))\n",
    "for conference in conferences_sorted_by_date:\n",
    "    print(f\"Conference: {conference.name}, Location: {conference.location}, Date: {conference.date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.quit()\n",
    "llm.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
